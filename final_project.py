# -*- coding: utf-8 -*-
"""final-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/razan-suleman/c21b3d9bfffe08299bd777283d17303c/final-project.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

import anvil.server

anvil.server.connect("server_T3K57EIKQOL3LRDFB7NMXA2H-HBYMLM4PH5HS7MQT")

import os
os.chdir("/content/drive/MyDrive/Final_Project")

# Read in data
import pandas as pd
df = pd.read_csv('data.csv')

!pip install pycountry

updated = df['Sex'] == 'Male'
df.loc[updated, 'Sex'] = 0
updated = df['Sex'] == 'Female'
df.loc[updated, 'Sex'] = 1

updated = df['Hemisphere'] == 'Southern Hemisphere'
df.loc[updated, 'Hemisphere'] = 1
updated = df['Hemisphere'] == 'Northern Hemisphere'
df.loc[updated, 'Hemisphere'] = 0

import pycountry
all_countries = list(pycountry.countries)

country_iso_map = {country.name: country.numeric for country in all_countries}
updated = df['Country'].isin(country_iso_map.keys())
df.loc[updated, 'Country'] = df.loc[updated, 'Country'].map(country_iso_map)

continent_mapping = {
    'Asia': 1,
    'Africa': 2,
    'North America': 3,
    'South America': 4,
    'Europe': 5,
    'Antarctica': 6,
    'Australia' : 7
}
df['Continent'] = df['Continent'].map(continent_mapping)

df['Sex'] = df['Sex'].apply(pd.to_numeric)

updated = df['Diet'] == 'Unhealthy'
df.loc[updated, 'Diet'] = 0
updated = df['Diet'] == 'Average'
df.loc[updated, 'Diet'] = 1
updated = df['Diet'] == 'Healthy'
df.loc[updated, 'Diet'] = 2

df['Diet'] = df['Diet'].apply(pd.to_numeric)

df['Country'].unique()
df['Country'] = df['Country'].replace('Vietnam', 84)
df['Country'] = df['Country'].replace('South Korea', 82)
df['Country'] = df['Country'].apply(pd.to_numeric)

df=df.drop('Patient ID', axis=1)

X_train = df.drop('Heart Attack Risk', axis=1).values  # Features (all columns except 'gets_admitted')
y_train = df['Heart Attack Risk'].values  # Target variable ('gets_admitted')\
y_train.shape

df['Hemisphere'] = df['Hemisphere'].apply(pd.to_numeric)

# Split the 'Blood Pressure' column into two new columns
df[['Systolic Blood Pressure', 'Diastolic Blood Pressure']] = df['Blood Pressure'].str.split('/', n=1, expand=True)
df=df.drop("Blood Pressure",axis=1)
df['Systolic Blood Pressure'] = df['Systolic Blood Pressure'].apply(pd.to_numeric)
df['Diastolic Blood Pressure'] = df['Diastolic Blood Pressure'].apply(pd.to_numeric)
df.info()

from sklearn.model_selection import train_test_split

X = df.drop('Heart Attack Risk', axis=1)
y = df['Heart Attack Risk']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)
print(y_test)

import numpy as np


def zscore_normalize_features(X):
    """
    מבצע נרמול זיכרוניות של כל המאפיינים בעמודת מאפיינים X.

    פרמטרים:
      X (ndarray (m,n))     : נתוני הקלט, m דוגמאות, n מאפיינים

    החזרה:
      X_norm (ndarray (m,n)): הקלט מנורמל לפי עמודה
      mu (ndarray (n,))     : הערך הממוצע של כל מאפיין
      sigma (ndarray (n,))  : הסטיית תקן של כל מאפיין
    """

    mu = np.mean(X, axis=0)
    sigma = np.std(X, axis=0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma


X_norm, mu, sigma = zscore_normalize_features(X_train)
print(f"X_mu = {mu}, \nX_sigma = {sigma}")
print(f"טווח מקסימלי לכל עמודה במקורי: {np.ptp(X_train,axis=0)}")
print(f"טווח מקסימלי לכל עמודה לאחר נרמול: {np.ptp(X_norm,axis=0)}")

df.corr(numeric_only=True)

df.drop(columns=["Sex" , "Sedentary Hours Per Day" , "BMI" , "Continent" , "Country" ] , axis=1 , inplace=True)

df.drop(columns=["Physical Activity Days Per Week" , "Medication Use", "Diet"] , axis=1 , inplace=True)

df.drop(columns=["Age" ,  "Heart Rate" , "Family History" , "Smoking" , "Previous Heart Problems" , "Stress Level" , "Income" , ] , axis=1 , inplace=True)

df.info()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from sklearn.datasets import load_iris
iris = load_iris()
X = df.drop('Heart Attack Risk', axis=1)
y = df['Heart Attack Risk']

print(X.shape, y.shape)
print(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
X_val.shape
X_train.shape

from keras import layers, models, regularizers
from keras.callbacks import EarlyStopping

# Define the model
model1 = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dropout(0.5),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(3, activation='softmax')
])



from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping

# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined


model1 = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model1.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model with early stopping
history1 = model1.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate the model
test_loss, test_accuracy = model1.evaluate(X_test, y_test)

print(f'Test Accuracy: {test_accuracy}, Test Loss: {test_loss}')

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# Predict the labels for the validation set
y_pred = model1.predict(X_val)
y_pred_classes = np.argmax(y_pred, axis=1)

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_val, y_pred_classes)
print("Confusion Matrix:")
print(conf_matrix)

# Adjust target names based on actual number of classes
num_classes = len(np.unique(y_val))
target_names = [f'Class {i}' for i in range(num_classes)]

# Calculate and print classification report
class_report = classification_report(y_val, y_pred_classes, target_names=target_names)
print("\nClassification Report:")
print(class_report)

# Precision, Recall, F1 Score
with np.errstate(divide='ignore', invalid='ignore'):
    Precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)
    Recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)
    F1_Score = 2 * (Precision * Recall) / (Precision + Recall)
    Precision[np.isnan(Precision)] = 0
    Recall[np.isnan(Recall)] = 0
    F1_Score[np.isnan(F1_Score)] = 0

print("Accuracy:", test_accuracy)
print("\nPrecision: ", Precision)
print("Recall: ", Recall)
print("F1 Score: ", F1_Score)





import numpy as np
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from scipy.stats import randint

# Define feature columns and target column
feature_columns = [col for col in df.columns if col != 'Heart Attack Risk']
X = df[feature_columns]
y = df['Heart Attack Risk']

# Split the data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Create polynomial features
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)
X_test_poly = poly.transform(X_test)

# Define the parameter distribution
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 4),
    'bootstrap': [True, False]
}

# Create a base model
rf = RandomForestClassifier(random_state=42)

# Instantiate the random search model
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,
                                   n_iter=50, cv=3, n_jobs=1, verbose=2, random_state=42, scoring='accuracy')

# Fit the random search to the data
random_search.fit(X_train_poly, y_train)

# Get the best model
best_rf = random_search.best_estimator_

# Evaluate on the validation set
val_accuracy = best_rf.score(X_val_poly, y_val)
print(f'Validation Accuracy: {val_accuracy}')

# Evaluate on the test set
test_accuracy = best_rf.score(X_test_poly, y_test)
print(f'Test Accuracy: {test_accuracy}')

i=0
for layer in model1.layers:
    i=i+1
    weights, biases = layer.get_weights()
    print("w", str(i), weights.shape, ";b", str(i), biases.shape)

# Plot training & validation accuracy values
plt.plot(history1.history['accuracy'])
plt.plot(history1.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history1.history['loss'])
plt.plot(history1.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

test_loss, test_acc = model1.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')

# Plot training & validation & test accuracy values
plt.plot(history1.history['accuracy'])
plt.plot(history1.history['val_accuracy'])
plt.scatter(len(history1.history['accuracy']) - 1, test_acc, label='Test', color='red') # Annotate test accuracy
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val', 'Test'], loc='lower right')
plt.show()

# Plot training & validation & test loss values
plt.plot(history1.history['loss'])
plt.plot(history1.history['val_loss'])
plt.scatter(len(history1.history['loss']) - 1, test_loss, label='Test', color='red') # Annotate test loss
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val', 'Test'], loc='upper right')
plt.show()

y_pred = np.argmax(model1.predict(X_test), axis=1)
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred = y_pred)



#Check your calulation with this code:
from sklearn import metrics, model_selection
print(metrics.classification_report(y_test, y_pred,labels=[0,1,2]))

model2 = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')
])

model2.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss',  # Monitor the validation loss
                               patience=10,         # Number of epochs with no improvement after which training will be stopped
                               verbose=1,          # Log level
                               mode='min',         # The training will stop when the quantity monitored has stopped decreasing
                               restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored quantity

# Include the callback in the model's fit method
history2 = model2.fit(X_train, y_train,
                      epochs=100,              # Set a high epoch number, early stopping will halt the training before reaching it
                      validation_data=(X_val, y_val),
                      callbacks=[early_stopping])

# Plot training & validation loss values
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# 'history2.history["loss"]' are list containing the loss values

# Plot the last 10 epochs for training loss
plt.figure(figsize=(10, 5))
plt.plot([1,2,3,4,5,6,7,8,9,10],history2.history['val_loss'][-10:], label='Validation Loss')
plt.title('Model loss over the last 10 epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.grid(linestyle='-', linewidth='0.5')
plt.xticks(ticks=np.arange(0, 11, 1))  # X-axis ticks from 0 to 10 with a step of 1
plt.yticks(ticks=np.linspace(plt.ylim()[0], plt.ylim()[1], 11))  # Y-axis ticks spaced evenly for the 10x10 grid

plt.show()

# YOUR CODE HERE
# Calculate conf_matrix
y_pred = np.argmax(model2.predict(X_test) , axis=1)
conf_matrix = confusion_matrix(y_test , y_pred)

# Print conf_matrix
print(conf_matrix)

# YOUR CODE HERE
# Calculate precision,recallf1_score
from sklearn.metrics import precision_score, recall_score, f1_score
precision = precision_score(y_test, y_pred, average=None)[0]
recall = recall_score(y_test, y_pred, average=None)[0]
f1_score = f1_score(y_test, y_pred, average=None)[0]
# Print precision,recallf1_score

print('Precision: ', precision)
print('Recall: ', recall)
print('F1 Score: ', f1_score)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers

# Build a simple model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)  # Output layer for regression (no activation function)
])

model.compile(optimizer='adam', loss='mean_squared_error')

model_history = model.fit(X_train, y_train, epochs=10, batch_size=32)

#print w,b shape in each layer
i=0
for layer in model.layers:
    i=i+1
    weights, biases = layer.get_weights()
    print("w", str(i), weights.shape, ";b", str(i), biases.shape)

print(model_history.history.keys())

# Plot training & validation loss values
plt.plot(model_history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper left')
plt.show()

# Evaluate the model
model_loss = model.evaluate(X_train, y_train)
print(f'model_loss: {model_loss}')

# Generate predictions on the test set
y_pred = model.predict(X_test)

# Calculate MSE
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss (MSE): {test_loss}")
from sklearn.metrics import mean_absolute_error, mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")

# Calculate MAE
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error (MAE): {mae}")

# Calculate RMSE (the square root of MSE)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error (RMSE): {rmse}")

@anvil.server.callable
def predict_iris(Cholesterol_box, Diabetes_box, Obesity_box, Alcohol_box, Exercise_box, Sleep_box, Hemisphere_box, Systolic_box, Diastolic_box):
  classification = knn.predict([[Cholesterol_box, Diabetes_box, Obesity_box, Alcohol_box, Exercise_box, Sleep_box, Hemisphere_box, Systolic_box, Diastolic_box]])
  return iris.target_names[classification][0]

anvil.server.wait_forever()

Connecting to wss://anvil.works/uplink
Anvil websocket open
Authenticated OK

"""תבדקו את המודל

LogisticRegression()

ותשוו את כל מדדי הערכה הבאים בין הרשת שפיתחתם לבין רגרסיה לוגיסטית:


    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)


"""